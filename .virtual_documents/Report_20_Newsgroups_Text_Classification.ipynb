# Importing required libraries
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.manifold import TSNE
import collections
import seaborn as sns
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import nltk
import gensim
import wordcloud
from sklearn import metrics
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import SGDClassifier
from sklearn.svm import SVC


# Defining matplotlib library Inline
get_ipython().run_line_magic("matplotlib", " inline")
# Defining globel size for plot
matplotlib.rcParams['figure.figsize'] = [16, 14]


# Fetching and Storing all data into news_dataset variable with shuffling
news_dataset = fetch_20newsgroups(subset='all',
                                  shuffle=True,
                                  remove=('headers', 'footers', 'quotes'))


# Printing target lables names
news_dataset.target_names


# Mapping labels with its integer values (Label Encoding)
lable_map = dict((i, j) for i, j in enumerate(news_dataset.target_names))
lable_map


# Converting train dataset into pandas dataframe and storing into train_df variable
df = pd.DataFrame({'data': news_dataset.data, 'target': news_dataset.target})
df["target_label"] = df["target"].map(lable_map)
df.head()


df.shape


# Printing dataset
df


# Dataset Information with columns and it's types
df.info()


# Using describe fuction to get statstics of dataset like Mean value, Standard Daviation
df.describe()


# Getting the exact count of Labels
df.target_label.value_counts()


# Ploting interactive graph of Label counts
sns.barplot(x=df["target_label"].value_counts().index,
            y=df["target_label"].value_counts().to_numpy())
plt.xticks(rotation=45)


# plot word count for data
wc = wordcloud.WordCloud()
wc.generate(str(df['data']))
fig = plt.figure(figsize=[20, 22])
plt.title('WordCloud of News')
plt.axis('off')
plt.imshow(wc)
plt.show()


# Converting text paragraph into tokens using nltk.work_tokenize function and also removing special charecters and un-usefull text
# And returning tokens list
def tokenize(data):
    tokens = nltk.word_tokenize(data)
    return [word for word in tokens if word.isalpha()]


# Applying tokenize function to data column of dataset and storing results into column name tokenized
df['tokenized'] = df.apply(lambda x: tokenize(x['data']), axis=1)
df.head()


# Definning function to remove english stopwords from tokenized data with the help of stopwords function of NLTK
def remove_stopword(data):

    stopword = stopwords.words("english")
    return [word for word in data if not word in stopword]


# Appling remove_stopword function to column tokenized of dataset and storing returned values into stopword_removed column

df['stopword_removed'] = df.apply(lambda x: remove_stopword(x['tokenized']),
                                  axis=1)
df[["data", "stopword_removed"]].head()


# Converting words to it's root form using stemming technique
# Word stemming using PorterStemmer function of NLTK
# And return the stemmed word list


def stemming(data):

    stemmer = PorterStemmer()
    return [stemmer.stem(word) for word in data]


# Appling stemming function to dataset with column name stopword_removed

df['stemmed_words'] = df.apply(lambda x: stemming(x['stopword_removed']),
                               axis=1)
df[['data', 'stemmed_words']].head()


# lemmatization also comvert words to its root form
# But the only difference between lemmatization and stemmeing is that stemming just removes or stems the last few characters of a word, often leading to incorrect meanings and spelling.
# lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma.


def lemmatization(data):
    lemmatizer = WordNetLemmatizer()
    return [lemmatizer.lemmatize(word) for word in data]


df['lemmatized_words'] = df.apply(lambda x: stemming(x['stopword_removed']),
                                  axis=1)
df[['data', 'lemmatized_words']].head()


# Function that join and return lemmatized words to string.
def word_rejoing(data):
    return (" ".join(data))


# Appling word_rejoing function to lemmatized_words and
# Stroing final string to dataframe with column name final_string
df['final_string'] = df.apply(lambda x: word_rejoing(x['lemmatized_words']),
                              axis=1)
df[['data', 'final_string']].head()


# plot wordcloud for final_string
wc = wordcloud.WordCloud()
wc.generate(str(df['final_string']))
fig = plt.figure(figsize=[20, 22])
plt.title('WordCloud of News')
plt.axis('off')
plt.imshow(wc)
plt.show()


temp = df["lemmatized_words"].to_numpy()
words = []
for lst in temp:
    words.extend(lst)

word_counts = collections.Counter(words)
word_counts = dict(sorted(word_counts.items(), key=lambda x: x[1],
                          reverse=True))


# Ploting first 100 most frequent words
sns.barplot(x=np.array(list(word_counts.keys()))[:100],
            y=np.array(list(word_counts.values()))[:100])
plt.xticks(rotation=45)


# to store the result of traning
tf_idf_train_acc = {}
# to store the result of test data
tf_idf_test_acc = {}


# Creating instance of TfidfVectorzer function of sklearn library
tf_idf = TfidfVectorizer()
# Converting final_strings to its meaning full words vectors from dataframe
# Storing final traing data to X variable
X = tf_idf.fit_transform(df["final_string"])
# Storing Labels to Y variable
Y = df["target"]
# Printing Shape of data
X.shape


# Displaying average non-zero values from dataset
X.nnz / float(X.shape[0])


# Printing array of X
X.toarray()


# t-SNE
tsne = TSNE(n_components=2, perplexity=50, learning_rate=300, n_iter=1800)
# tsne to our document vectors
componets = tsne.fit_transform(X)


# plot TI-IDF vectors for final_string data
def plot_TFIDF_embeddings(embedding, title):
    fig = plt.figure(figsize=[15, 12])
    ax = sns.scatterplot(embedding[:, 0],
                         embedding[:, 1],
                         hue=df['target_label'])
    plt.title(title)
    plt.xlabel('axis 0')
    plt.ylabel('axis 1')
    plt.legend(bbox_to_anchor=(1.05, 1), loc=2)
    plt.show()


plot_TFIDF_embeddings(componets, 'Visualizing TFIDF vectors (t-SNE)')


# Spliting dataset into trian test data with ratio 80:20 with suffuling
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    Y,
                                                    test_size=0.2,
                                                    random_state=42)


# Defining naive bayes classifier model with MultinomialNB function of sklearn
# fitting train data into model

naive_bayes_classifier = MultinomialNB()
naive_bayes_classifier.fit(X_train, y_train)


# getting training accuracy of model
y_pred = naive_bayes_classifier.predict(X_train)
NB_train_acc = metrics.accuracy_score(y_train, y_pred)
print("Train Accuracy : ", NB_train_acc)
tf_idf_train_acc["NaiveBayes"] = NB_train_acc

# Performing prediction on test dataset
y_pred = naive_bayes_classifier.predict(X_test)
NB_test_acc = metrics.accuracy_score(y_test, y_pred)
print("Test Accuracy : ", NB_test_acc)
tf_idf_test_acc["NaiveBayes"] = NB_test_acc


# Getting full report of model classification that how it performed
print(metrics.classification_report(y_test, y_pred))


# Ploting confusion matrix with interective graphics
confusion_matrix = metrics.confusion_matrix(y_test, y_pred)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix)

cm_display.plot()
plt.show()


# Creating model of SGD Classifier with SGDClassifier class of sklearn
# Fitting the traing dataset into model
sgd = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42)
sgd.fit(X_train, y_train)


# getting training accuracy of model
y_pred = sgd.predict(X_train)
SGD_train_acc = metrics.accuracy_score(y_train, y_pred)
print("Train Accuracy : ", SGD_train_acc)
tf_idf_train_acc["SGD"] = SGD_train_acc

# Performing prediction on test dataset
y_pred = sgd.predict(X_test)
SGD_test_acc = metrics.accuracy_score(y_test, y_pred)
print("Test Accuracy : ", SGD_test_acc)
tf_idf_test_acc["SGD"] = SGD_test_acc


print(metrics.classification_report(y_test, y_pred))


# Ploting confusion matrix with interective graphics
confusion_matrix = metrics.confusion_matrix(y_test, y_pred)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix)

cm_display.plot()
plt.show()


svm = SVC()
svm.fit(X_train, y_train)


# getting training accuracy of model
y_pred = svm.predict(X_train)
SVM_train_acc = metrics.accuracy_score(y_train, y_pred)
print("Train Accuracy : ", SVM_train_acc)
tf_idf_train_acc["SVM"] = SVM_train_acc

# Performing prediction on test dataset
y_pred = svm.predict(X_test)
SVM_test_acc = metrics.accuracy_score(y_test, y_pred)
print("Test Accuracy : ", SVM_test_acc)
tf_idf_test_acc["SVM"] = SVM_test_acc


# Ploting confusion matrix with interective graphics
confusion_matrix = metrics.confusion_matrix(y_test, y_pred)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix)

cm_display.plot()
plt.show()


tf_idf_test_acc, tf_idf_train_acc


X = list(tf_idf_train_acc.keys())
train = list(tf_idf_train_acc.values())
test = list(tf_idf_test_acc.values())

X_axis = np.arange(len(X))

plt.bar(X_axis - 0.2, train, 0.4, label='Train')
plt.bar(X_axis + 0.2, test, 0.4, label='Test')
plt.xticks(X_axis, X)
plt.xlabel("Classifiers")
plt.ylabel("Accuracy")
plt.title("Accuracy Summary")
plt.legend()
plt.show()


w2v_train_acc = {}
w2v_test_acc = {}


# Splitting the lemmatized_wrods list into train test dataset
X_train, X_test, y_train, y_test = train_test_split(df['lemmatized_words'],
                                                    df['target'],
                                                    test_size=0.2)


# Train the word2vec model with vector_size 800 and window size 600
w2v_model = gensim.models.Word2Vec(X_train,
                                   vector_size=500,
                                   window=100,
                                   min_count=20)


def create_tsne_plot(model):
    "Creates and TSNE model and plots it"
    labels = []
    tokens = []

    for word in list(model.wv.index_to_key):
        tokens.append(model.wv[word])
        labels.append(word)

    tsne_model = TSNE(perplexity=40,
                      n_components=2,
                      init='pca',
                      n_iter=1000,
                      random_state=20)
    new_values = tsne_model.fit_transform(tokens)

    x = []
    y = []
    for value in new_values:
        x.append(value[0])
        y.append(value[1])

    plt.figure(figsize=(20, 16))
    for i in range(len(x)):
        plt.scatter(x[i], y[i])
        plt.annotate(labels[i],
                     xy=(x[i], y[i]),
                     xytext=(5, 2),
                     textcoords='offset points',
                     ha='right',
                     va='bottom')
    plt.show()


create_tsne_plot(w2v_model)


# Generating aggregated sentence vectors based on the word vectors for each word in the sentence
words = set(w2v_model.wv.index_to_key)
X_train_vect = np.array(
    [np.array([w2v_model.wv[i] for i in ls if i in words]) for ls in X_train])
X_test_vect = np.array(
    [np.array([w2v_model.wv[i] for i in ls if i in words]) for ls in X_test])


# Compute sentence vectors by averaging the word vectors for the words contained in the sentence
X_train_vect_avg = []
for v in X_train_vect:
    if v.size:
        X_train_vect_avg.append(v.mean(axis=0))
    else:
        X_train_vect_avg.append(np.zeros(500, dtype=float))

X_test_vect_avg = []
for v in X_test_vect:
    if v.size:
        X_test_vect_avg.append(v.mean(axis=0))
    else:
        X_test_vect_avg.append(np.zeros(500, dtype=float))


# Creating SGD Classifier model and feeding training data to it
sgd = SGDClassifier()
sgd.fit(X_train_vect_avg, y_train)


# getting training accuracy of model
y_pred = sgd.predict(X_train_vect_avg)
SGD_train_acc = metrics.accuracy_score(y_train, y_pred)
print("Train Accuracy : ", SGD_train_acc)
w2v_train_acc["SGD"] = SGD_train_acc

# Performing prediction on test dataset
y_pred = sgd.predict(X_test_vect_avg)
SGD_test_acc = metrics.accuracy_score(y_test, y_pred)
print("Test Accuracy : ", SGD_test_acc)
w2v_test_acc["SGD"] = SGD_test_acc


# Displaying report of prediction
print(metrics.classification_report(y_test, y_pred))


# Train data with randomforest classifer
rf = RandomForestClassifier()
rf_model = rf.fit(X_train_vect_avg, y_train)


# getting training accuracy of model
y_pred = rf_model.predict(X_train_vect_avg)
rf_train_acc = metrics.accuracy_score(y_train, y_pred)
print("Train Accuracy : ", rf_train_acc)
w2v_train_acc["RandomForest"] = rf_train_acc

# Performing prediction on test dataset
y_pred = rf_model.predict(X_test_vect_avg)
rf_test_acc = metrics.accuracy_score(y_test, y_pred)
print("Test Accuracy : ", rf_test_acc)
w2v_test_acc["RandomForest"] = rf_test_acc


print(metrics.classification_report(y_test, y_pred))


svm = SVC()
svm.fit(X_train_vect_avg, y_train)


# getting training accuracy of model
y_pred = svm.predict(X_train_vect_avg)
SVM_train_acc = metrics.accuracy_score(y_train, y_pred)
print("Train Accuracy : ", SVM_train_acc)
w2v_train_acc["SVM"] = SVM_train_acc

# Performing prediction on test dataset
y_pred = svm.predict(X_test_vect_avg)
SVM_test_acc = metrics.accuracy_score(y_test, y_pred)
print("Test Accuracy : ", SVM_test_acc)
w2v_test_acc["SVM"] = SVM_test_acc


print(metrics.classification_report(y_test, y_pred))


# Comapring resuls of all models
X = list(w2v_train_acc.keys())
train = list(w2v_train_acc.values())
test = list(w2v_test_acc.values())

X_axis = np.arange(len(X))

plt.bar(X_axis - 0.2, train, 0.4, label='Train')
plt.bar(X_axis + 0.2, test, 0.4, label='Test')
plt.xticks(X_axis, X)
plt.xlabel("Classifiers")
plt.ylabel("Accuracy")
plt.title("Accuracy Summary")
plt.legend()
plt.show()
